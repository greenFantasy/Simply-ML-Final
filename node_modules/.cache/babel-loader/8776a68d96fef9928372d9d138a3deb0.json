{"ast":null,"code":"import { MAIN } from '../../data';\nimport { fieldIntersection, hash, hasIntersection, keys, some } from '../../util';\nimport { AggregateNode } from './aggregate';\nimport { BinNode } from './bin';\nimport { OutputNode } from './dataflow';\nimport { FacetNode } from './facet';\nimport { FilterNode } from './filter';\nimport { ParseNode } from './formatparse';\nimport { JoinAggregateTransformNode } from './joinaggregate';\nimport { FACET_SCALE_PREFIX } from './optimize';\nimport { BottomUpOptimizer, isDataSourceNode, TopDownOptimizer } from './optimizer';\nimport { StackNode } from './stack';\nimport { TimeUnitNode } from './timeunit';\nimport { WindowTransformNode } from './window';\n/**\n * Move parse nodes up to forks.\n */\n\nexport class MoveParseUp extends BottomUpOptimizer {\n  run(node) {\n    const parent = node.parent; // Move parse up by merging or swapping.\n\n    if (node instanceof ParseNode) {\n      if (isDataSourceNode(parent)) {\n        return this.flags;\n      }\n\n      if (parent.numChildren() > 1) {\n        // Don't move parse further up but continue with parent.\n        this.setContinue();\n        return this.flags;\n      }\n\n      if (parent instanceof ParseNode) {\n        this.setMutated();\n        parent.merge(node);\n      } else {\n        // Don't swap with nodes that produce something that the parse node depends on (e.g. lookup).\n        if (fieldIntersection(parent.producedFields(), node.dependentFields())) {\n          this.setContinue();\n          return this.flags;\n        }\n\n        this.setMutated();\n        node.swapWithParent();\n      }\n    }\n\n    this.setContinue();\n    return this.flags;\n  }\n\n}\n/**\n * Merge identical nodes at forks by comparing hashes.\n *\n * Does not need to iterate from leaves so we implement this with recursion as it's a bit simpler.\n */\n\nexport class MergeIdenticalNodes extends TopDownOptimizer {\n  mergeNodes(parent, nodes) {\n    const mergedNode = nodes.shift();\n\n    for (const node of nodes) {\n      parent.removeChild(node);\n      node.parent = mergedNode;\n      node.remove();\n    }\n  }\n\n  run(node) {\n    const hashes = node.children.map(x => x.hash());\n    const buckets = {};\n\n    for (let i = 0; i < hashes.length; i++) {\n      if (buckets[hashes[i]] === undefined) {\n        buckets[hashes[i]] = [node.children[i]];\n      } else {\n        buckets[hashes[i]].push(node.children[i]);\n      }\n    }\n\n    for (const k of keys(buckets)) {\n      if (buckets[k].length > 1) {\n        this.setMutated();\n        this.mergeNodes(node, buckets[k]);\n      }\n    }\n\n    for (const child of node.children) {\n      this.run(child);\n    }\n\n    return this.mutatedFlag;\n  }\n\n}\n/**\n * Repeatedly remove leaf nodes that are not output or facet nodes.\n * The reason is that we don't need subtrees that don't have any output nodes.\n * Facet nodes are needed for the row or column domains.\n */\n\nexport class RemoveUnusedSubtrees extends BottomUpOptimizer {\n  run(node) {\n    if (node instanceof OutputNode || node.numChildren() > 0 || node instanceof FacetNode) {\n      // no need to continue with parent because it is output node or will have children (there was a fork)\n      return this.flags;\n    } else {\n      this.setMutated();\n      node.remove();\n    }\n\n    return this.flags;\n  }\n\n}\n/**\n * Removes duplicate time unit nodes (as determined by the name of the\n * output field) that may be generated due to selections projected over\n * time units.\n */\n\nexport class RemoveDuplicateTimeUnits extends BottomUpOptimizer {\n  constructor() {\n    super(...arguments);\n    this.fields = new Set();\n  }\n\n  run(node) {\n    this.setContinue();\n\n    if (node instanceof TimeUnitNode) {\n      const pfields = node.producedFields();\n\n      if (hasIntersection(pfields, this.fields)) {\n        this.setMutated();\n        node.remove();\n      } else {\n        this.fields = new Set([...this.fields, ...pfields]);\n      }\n    }\n\n    return this.flags;\n  }\n\n  reset() {\n    this.fields.clear();\n  }\n\n}\n/**\n * Merge adjacent time unit nodes.\n */\n\nexport class MergeTimeUnits extends BottomUpOptimizer {\n  run(node) {\n    this.setContinue();\n    const parent = node.parent;\n    const timeUnitChildren = parent.children.filter(x => x instanceof TimeUnitNode);\n    const combination = timeUnitChildren.pop();\n\n    for (const timeUnit of timeUnitChildren) {\n      this.setMutated();\n      combination.merge(timeUnit);\n    }\n\n    return this.flags;\n  }\n\n}\n/**\n * Clones the subtree and ignores output nodes except for the leaves, which are renamed.\n */\n\nfunction cloneSubtree(facet) {\n  function clone(node) {\n    if (!(node instanceof FacetNode)) {\n      const copy = node.clone();\n\n      if (copy instanceof OutputNode) {\n        const newName = FACET_SCALE_PREFIX + copy.getSource();\n        copy.setSource(newName);\n        facet.model.component.data.outputNodes[newName] = copy;\n      } else if (copy instanceof AggregateNode || copy instanceof StackNode || copy instanceof WindowTransformNode || copy instanceof JoinAggregateTransformNode) {\n        copy.addDimensions(facet.fields);\n      }\n\n      node.children.flatMap(clone).forEach(n => n.parent = copy);\n      return [copy];\n    }\n\n    return node.children.flatMap(clone);\n  }\n\n  return clone;\n}\n/**\n * Move facet nodes down to the next fork or output node. Also pull the main output with the facet node.\n * After moving down the facet node, make a copy of the subtree and make it a child of the main output.\n */\n\n\nexport function moveFacetDown(node) {\n  if (node instanceof FacetNode) {\n    if (node.numChildren() === 1 && !(node.children[0] instanceof OutputNode)) {\n      // move down until we hit a fork or output node\n      const child = node.children[0];\n\n      if (child instanceof AggregateNode || child instanceof StackNode || child instanceof WindowTransformNode || child instanceof JoinAggregateTransformNode) {\n        child.addDimensions(node.fields);\n      }\n\n      child.swapWithParent();\n      moveFacetDown(node);\n    } else {\n      // move main to facet\n      const facetMain = node.model.component.data.main;\n      moveMainDownToFacet(facetMain); // replicate the subtree and place it before the facet's main node\n\n      const cloner = cloneSubtree(node);\n      const copy = node.children.map(cloner).flat();\n\n      for (const c of copy) {\n        c.parent = facetMain;\n      }\n    }\n  } else {\n    node.children.map(moveFacetDown);\n  }\n}\n\nfunction moveMainDownToFacet(node) {\n  if (node instanceof OutputNode && node.type === MAIN) {\n    if (node.numChildren() === 1) {\n      const child = node.children[0];\n\n      if (!(child instanceof FacetNode)) {\n        child.swapWithParent();\n        moveMainDownToFacet(node);\n      }\n    }\n  }\n}\n/**\n * Remove nodes that are not required starting from a root.\n */\n\n\nexport class RemoveUnnecessaryNodes extends TopDownOptimizer {\n  run(node) {\n    // remove output nodes that are not required\n    if (node instanceof OutputNode && !node.isRequired()) {\n      this.setMutated();\n      node.remove();\n    }\n\n    for (const child of node.children) {\n      this.run(child);\n    }\n\n    return this.mutatedFlag;\n  }\n\n}\n/**\n * Inserts an intermediate ParseNode containing all non-conflicting parse fields and removes the empty ParseNodes.\n *\n * We assume that dependent paths that do not have a parse node can be just merged.\n */\n\nexport class MergeParse extends BottomUpOptimizer {\n  run(node) {\n    const parent = node.parent;\n    const originalChildren = [...parent.children];\n    const parseChildren = parent.children.filter(child => child instanceof ParseNode);\n\n    if (parent.numChildren() > 1 && parseChildren.length >= 1) {\n      const commonParse = {};\n      const conflictingParse = new Set();\n\n      for (const parseNode of parseChildren) {\n        const parse = parseNode.parse;\n\n        for (const k of keys(parse)) {\n          if (!(k in commonParse)) {\n            commonParse[k] = parse[k];\n          } else if (commonParse[k] !== parse[k]) {\n            conflictingParse.add(k);\n          }\n        }\n      }\n\n      for (const field of conflictingParse) {\n        delete commonParse[field];\n      }\n\n      if (keys(commonParse).length !== 0) {\n        this.setMutated();\n        const mergedParseNode = new ParseNode(parent, commonParse);\n\n        for (const childNode of originalChildren) {\n          if (childNode instanceof ParseNode) {\n            for (const key of keys(commonParse)) {\n              delete childNode.parse[key];\n            }\n          }\n\n          parent.removeChild(childNode);\n          childNode.parent = mergedParseNode; // remove empty parse nodes\n\n          if (childNode instanceof ParseNode && keys(childNode.parse).length === 0) {\n            childNode.remove();\n          }\n        }\n      }\n    }\n\n    this.setContinue();\n    return this.flags;\n  }\n\n}\nexport class MergeAggregates extends BottomUpOptimizer {\n  run(node) {\n    const parent = node.parent;\n    const aggChildren = parent.children.filter(child => child instanceof AggregateNode); // Object which we'll use to map the fields which an aggregate is grouped by to\n    // the set of aggregates with that grouping. This is useful as only aggregates\n    // with the same group by can be merged\n\n    const groupedAggregates = {}; // Build groupedAggregates\n\n    for (const agg of aggChildren) {\n      const groupBys = hash(agg.groupBy);\n\n      if (!(groupBys in groupedAggregates)) {\n        groupedAggregates[groupBys] = [];\n      }\n\n      groupedAggregates[groupBys].push(agg);\n    } // Merge aggregateNodes with same key in groupedAggregates\n\n\n    for (const group of keys(groupedAggregates)) {\n      const mergeableAggs = groupedAggregates[group];\n\n      if (mergeableAggs.length > 1) {\n        const mergedAggs = mergeableAggs.pop();\n\n        for (const agg of mergeableAggs) {\n          if (mergedAggs.merge(agg)) {\n            parent.removeChild(agg);\n            agg.parent = mergedAggs;\n            agg.remove();\n            this.setMutated();\n          }\n        }\n      }\n    }\n\n    this.setContinue();\n    return this.flags;\n  }\n\n}\n/**\n * Merge bin nodes and move them up through forks. Stop at filters and parse as we want them to stay before the bin node.\n */\n\nexport class MergeBins extends BottomUpOptimizer {\n  constructor(model) {\n    super();\n    this.model = model;\n  }\n\n  run(node) {\n    const parent = node.parent;\n    const moveBinsUp = !(isDataSourceNode(parent) || parent instanceof FilterNode || parent instanceof ParseNode);\n    const promotableBins = [];\n    const remainingBins = [];\n\n    for (const child of parent.children) {\n      if (child instanceof BinNode) {\n        if (moveBinsUp && !fieldIntersection(parent.producedFields(), child.dependentFields())) {\n          promotableBins.push(child);\n        } else {\n          remainingBins.push(child);\n        }\n      }\n    }\n\n    if (promotableBins.length > 0) {\n      const promotedBin = promotableBins.pop();\n\n      for (const bin of promotableBins) {\n        promotedBin.merge(bin, this.model.renameSignal.bind(this.model));\n      }\n\n      this.setMutated();\n\n      if (parent instanceof BinNode) {\n        parent.merge(promotedBin, this.model.renameSignal.bind(this.model));\n      } else {\n        promotedBin.swapWithParent();\n      }\n    }\n\n    if (remainingBins.length > 1) {\n      const remainingBin = remainingBins.pop();\n\n      for (const bin of remainingBins) {\n        remainingBin.merge(bin, this.model.renameSignal.bind(this.model));\n      }\n\n      this.setMutated();\n    }\n\n    this.setContinue();\n    return this.flags;\n  }\n\n}\n/**\n * This optimizer takes output nodes that are at a fork and moves them before the fork.\n *\n * The algorithm iterates over the children and tries to find the last output node in a cahin of output nodes.\n * It then moves all output nodes before that main output node. All other children (and the children of the output nodes)\n * are inserted after the main output node.\n */\n\nexport class MergeOutputs extends BottomUpOptimizer {\n  run(node) {\n    const parent = node.parent;\n    const children = [...parent.children];\n    const hasOutputChild = some(children, child => child instanceof OutputNode);\n\n    if (!hasOutputChild || parent.numChildren() <= 1) {\n      this.setContinue();\n      return this.flags;\n    }\n\n    const otherChildren = []; // The output node we will connect all other nodes to\n    // output nodes will be added before, other nodes after\n\n    let mainOutput;\n\n    for (const child of children) {\n      if (child instanceof OutputNode) {\n        let lastOutput = child;\n\n        while (lastOutput.numChildren() === 1) {\n          const theChild = lastOutput.children[0];\n\n          if (theChild instanceof OutputNode) {\n            lastOutput = theChild;\n          } else {\n            break;\n          }\n        }\n\n        otherChildren.push(...lastOutput.children);\n\n        if (mainOutput) {\n          // Move the output nodes before the mainOutput. We do this by setting\n          // the parent of the first not to the parent of the main output and\n          // the main output's parent to the last output.\n          // note: the child is the first output\n          parent.removeChild(child);\n          child.parent = mainOutput.parent;\n          mainOutput.parent.removeChild(mainOutput);\n          mainOutput.parent = lastOutput;\n          this.setMutated();\n        } else {\n          mainOutput = lastOutput;\n        }\n      } else {\n        otherChildren.push(child);\n      }\n    }\n\n    if (otherChildren.length) {\n      this.setMutated();\n\n      for (const child of otherChildren) {\n        child.parent.removeChild(child);\n        child.parent = mainOutput;\n      }\n    }\n\n    this.setContinue();\n    return this.flags;\n  }\n\n}","map":null,"metadata":{},"sourceType":"module"}